{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset and set up the data folder"
      ],
      "metadata": {
        "id": "ih2ZFO4BZA92"
      },
      "id": "ih2ZFO4BZA92"
    },
    {
      "cell_type": "code",
      "source": [
        "## Download the dataset from github repo\n",
        "!wget https://media.githubusercontent.com/media/SIGCOMM22-5GMobility/artifact/dev/raw-data/D1.csv\n",
        "!wget https://media.githubusercontent.com/media/SIGCOMM22-5GMobility/artifact/dev/raw-data/D2.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAyG5aQ9YvnF",
        "outputId": "67be6bcd-d575-498c-e699-a3334edb3acd"
      },
      "id": "XAyG5aQ9YvnF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-26 00:15:42--  https://media.githubusercontent.com/media/SIGCOMM22-5GMobility/artifact/dev/raw-data/D1.csv\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345077566 (329M) [text/plain]\n",
            "Saving to: ‘D1.csv’\n",
            "\n",
            "D1.csv              100%[===================>] 329.09M   283MB/s    in 1.2s    \n",
            "\n",
            "2022-06-26 00:15:44 (283 MB/s) - ‘D1.csv’ saved [345077566/345077566]\n",
            "\n",
            "--2022-06-26 00:15:44--  https://media.githubusercontent.com/media/SIGCOMM22-5GMobility/artifact/dev/raw-data/D2.csv\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 293077812 (280M) [text/plain]\n",
            "Saving to: ‘D2.csv’\n",
            "\n",
            "D2.csv              100%[===================>] 279.50M   254MB/s    in 1.1s    \n",
            "\n",
            "2022-06-26 00:15:55 (254 MB/s) - ‘D2.csv’ saved [293077812/293077812]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## configure directory where data is stored\n",
        "import os\n",
        "DATA_FOLDER = os.path.curdir"
      ],
      "metadata": {
        "id": "_OAcYEJOTc6m"
      },
      "id": "_OAcYEJOTc6m",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "considered-component",
      "metadata": {
        "id": "considered-component"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "applied-payment",
      "metadata": {
        "id": "applied-payment"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import random\n",
        "import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score, plot_roc_curve\n",
        "\n",
        "# from utils.context import data_dir_shared_pred\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option('display.float_format',lambda x : '%.3f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the parameters for data processing and training\n",
        "\n",
        "```Split method```:\n",
        "Split the whole database into split 1 and split 2 based on the ratio. Do the negative sampling on split 1 for training. Test the model on split 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lt7kAMOjgG13"
      },
      "id": "Lt7kAMOjgG13"
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "### Config\n",
        "####################################\n",
        "PARAMETERS = {}\n",
        "\n",
        "# Read Database\n",
        "PARAMETERS[\"data_folder\"] = DATA_FOLDER\n",
        "PARAMETERS[\"file_name\"] = \"D1\"\n",
        "# PARAMETERS[\"file_name\"] = \"D2\"\n",
        "\n",
        "# Task\n",
        "PARAMETERS[\"label\"] = \"all_ho_category\"\n",
        "PARAMETERS[\"binary\"] = True\n",
        "PARAMETERS[\"feature_list\"] = [\"lon\", \"lat\", 'speed', 'lte_serv_rsrp', 'lte_serv_rsrq', 'lte_serv_rssi','lte_nbr_rsrp', 'lte_nbr_rsrq', 'lte_cqi0', 'lte_cqi1', 'dl_tput', \\\n",
        "                              'nr_serv_rsrp', 'nr_serv_rsrq', 'nr_serv_sinr', 'nr_cqi', 'nr_nbr_rsrp', 'nr_nbr_rsrq']  # , 'irat_rsrp', 'irat_rsrq'\n",
        "\n",
        "# Process the dataset \n",
        "PARAMETERS[\"split_mode\"] = \"train_neg_sampling\"\n",
        "\n",
        "####################################\n",
        "### Config (Hyper-parameters) \n",
        "####################################\n",
        "# Read Database\n",
        "PARAMETERS[\"sample_rate\"] = 50  # ms \n",
        "\n",
        "# Process the dataset \n",
        "PARAMETERS[\"train_ratio\"] = 0.7\n",
        "PARAMETERS[\"hist_window\"] = 20\n",
        "PARAMETERS[\"pred_window\"] = 20\n",
        "if PARAMETERS[\"file_name\"] == \"D1\":\n",
        "  PARAMETERS[\"neg_size\"] = 10000\n",
        "elif PARAMETERS[\"file_name\"] == \"D2\":\n",
        "  PARAMETERS[\"neg_size\"] = 50000\n",
        "  \n",
        "# Machine Learning \n",
        "PARAMETERS[\"batch_size\"] = 64\n",
        "PARAMETERS[\"epochs\"] = 200\n",
        "PARAMETERS[\"latent_dim\"] = 32"
      ],
      "metadata": {
        "id": "JUhq4c_sY5tY"
      },
      "id": "JUhq4c_sY5tY",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data\n",
        "It will take a couple of minutes (D2 will take 15min in Colab)."
      ],
      "metadata": {
        "id": "AZAyl5wqbm7v"
      },
      "id": "AZAyl5wqbm7v"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "verified-angel",
      "metadata": {
        "id": "verified-angel",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "06b146f2-9c6d-40b2-c145-80733aa98707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['TIME_STAMP', 'lon', 'lat', 'speed', 'technology', 'lte_band',\n",
            "       'lte_serv_rsrp', 'lte_serv_rsrq', 'lte_serv_rssi', 'lte_cqi0',\n",
            "       'lte_cqi1', 'nr_band', 'nr_serv_rsrp', 'nr_serv_rsrq', 'nr_serv_sinr',\n",
            "       'nr_cqi', 'dl_tput', 'ul_tput', 'lte_nbr_rsrp', 'lte_nbr_rsrq',\n",
            "       'irat_rsrp', 'irat_rsrq', 'nr_nbr_rsrp', 'nr_nbr_rsrq',\n",
            "       'lte_ho_category', 'nr_ho_category', 'OLD_TIME_STAMP',\n",
            "       'all_ho_category', 'r_time'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          r_time     lon    lat  speed  lte_serv_rsrp  lte_serv_rsrq  \\\n",
              "0              0 -71.053 42.367  3.000        -76.240        -10.016   \n",
              "1             50 -71.053 42.367  3.000        -76.393         -9.599   \n",
              "2            100 -71.053 42.367  3.000        -77.722        -10.073   \n",
              "3            150 -71.053 42.367  3.000        -75.928         -9.228   \n",
              "4            200 -71.053 42.367  3.000        -75.340         -9.689   \n",
              "...          ...     ...    ...    ...            ...            ...   \n",
              "317855  15892750 -71.053 42.367  0.000        -69.242         -8.337   \n",
              "317856  15892800 -71.053 42.367  0.000        -69.242         -8.337   \n",
              "317857  15892850 -71.053 42.367  0.000        -69.242         -8.337   \n",
              "317858  15892900 -71.053 42.367  0.000        -68.890         -8.481   \n",
              "317859  15892950 -71.053 42.367  0.000        -68.890         -8.481   \n",
              "\n",
              "        lte_serv_rssi  lte_cqi0  lte_cqi1  nr_serv_rsrp  ...  ul_tput  \\\n",
              "0             -46.430    14.000     0.000       -89.630  ...    2.034   \n",
              "1             -47.974    14.000     0.000       -89.630  ...    2.034   \n",
              "2             -48.718    14.000     0.000       -90.310  ...    2.034   \n",
              "3             -44.980    14.000     0.000       -90.310  ...    2.214   \n",
              "4             -45.404    11.429     0.000       -90.310  ...    2.250   \n",
              "...               ...       ...       ...           ...  ...      ...   \n",
              "317855        -39.939     7.381     8.571       -86.680  ...    0.000   \n",
              "317856        -39.939     7.381     8.571       -86.680  ...    0.000   \n",
              "317857        -39.939     7.381     8.571       -86.680  ...    0.000   \n",
              "317858        -39.912    10.048     5.571       -86.680  ...    0.000   \n",
              "317859        -39.912    10.048     5.571       -86.680  ...    0.000   \n",
              "\n",
              "        lte_nbr_rsrp  lte_nbr_rsrq  irat_rsrp  irat_rsrq  nr_nbr_rsrp  \\\n",
              "0            -79.940       -15.630    -96.010    -10.510     -117.330   \n",
              "1            -79.940       -15.630    -96.010    -10.510     -117.330   \n",
              "2            -79.940       -15.630    -96.010    -10.510     -116.840   \n",
              "3            -80.420       -14.273    -96.010    -10.510     -116.840   \n",
              "4            -81.380       -11.560    -96.010    -10.510     -116.840   \n",
              "...              ...           ...        ...        ...          ...   \n",
              "317855       -80.758       -17.966    -99.500    -10.510     -109.700   \n",
              "317856       -80.758       -17.966    -99.500    -10.510     -109.700   \n",
              "317857       -80.758       -17.966    -99.500    -10.510     -109.700   \n",
              "317858       -80.750       -16.001    -99.500    -10.510     -109.700   \n",
              "317859       -80.750       -16.001    -99.500    -10.510     -109.700   \n",
              "\n",
              "        nr_nbr_rsrq  lte_ho_category  nr_ho_category  all_ho_category  \n",
              "0           -16.590            0.000           0.000            0.000  \n",
              "1           -16.590            0.000           0.000            0.000  \n",
              "2           -16.050            0.000           0.000            0.000  \n",
              "3           -16.050            0.000           0.000            0.000  \n",
              "4           -16.050            0.000           0.000            0.000  \n",
              "...             ...              ...             ...              ...  \n",
              "317855      -12.280            0.000           0.000            0.000  \n",
              "317856      -12.280            0.000           0.000            0.000  \n",
              "317857      -12.280            0.000           0.000            0.000  \n",
              "317858      -12.280            0.000           0.000            0.000  \n",
              "317859      -12.280            0.000           0.000            0.000  \n",
              "\n",
              "[317860 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-543783e6-b40e-442d-8a14-5ad99225f8ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r_time</th>\n",
              "      <th>lon</th>\n",
              "      <th>lat</th>\n",
              "      <th>speed</th>\n",
              "      <th>lte_serv_rsrp</th>\n",
              "      <th>lte_serv_rsrq</th>\n",
              "      <th>lte_serv_rssi</th>\n",
              "      <th>lte_cqi0</th>\n",
              "      <th>lte_cqi1</th>\n",
              "      <th>nr_serv_rsrp</th>\n",
              "      <th>...</th>\n",
              "      <th>ul_tput</th>\n",
              "      <th>lte_nbr_rsrp</th>\n",
              "      <th>lte_nbr_rsrq</th>\n",
              "      <th>irat_rsrp</th>\n",
              "      <th>irat_rsrq</th>\n",
              "      <th>nr_nbr_rsrp</th>\n",
              "      <th>nr_nbr_rsrq</th>\n",
              "      <th>lte_ho_category</th>\n",
              "      <th>nr_ho_category</th>\n",
              "      <th>all_ho_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>3.000</td>\n",
              "      <td>-76.240</td>\n",
              "      <td>-10.016</td>\n",
              "      <td>-46.430</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-89.630</td>\n",
              "      <td>...</td>\n",
              "      <td>2.034</td>\n",
              "      <td>-79.940</td>\n",
              "      <td>-15.630</td>\n",
              "      <td>-96.010</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-117.330</td>\n",
              "      <td>-16.590</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>3.000</td>\n",
              "      <td>-76.393</td>\n",
              "      <td>-9.599</td>\n",
              "      <td>-47.974</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-89.630</td>\n",
              "      <td>...</td>\n",
              "      <td>2.034</td>\n",
              "      <td>-79.940</td>\n",
              "      <td>-15.630</td>\n",
              "      <td>-96.010</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-117.330</td>\n",
              "      <td>-16.590</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>3.000</td>\n",
              "      <td>-77.722</td>\n",
              "      <td>-10.073</td>\n",
              "      <td>-48.718</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-90.310</td>\n",
              "      <td>...</td>\n",
              "      <td>2.034</td>\n",
              "      <td>-79.940</td>\n",
              "      <td>-15.630</td>\n",
              "      <td>-96.010</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-116.840</td>\n",
              "      <td>-16.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>150</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>3.000</td>\n",
              "      <td>-75.928</td>\n",
              "      <td>-9.228</td>\n",
              "      <td>-44.980</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-90.310</td>\n",
              "      <td>...</td>\n",
              "      <td>2.214</td>\n",
              "      <td>-80.420</td>\n",
              "      <td>-14.273</td>\n",
              "      <td>-96.010</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-116.840</td>\n",
              "      <td>-16.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>3.000</td>\n",
              "      <td>-75.340</td>\n",
              "      <td>-9.689</td>\n",
              "      <td>-45.404</td>\n",
              "      <td>11.429</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-90.310</td>\n",
              "      <td>...</td>\n",
              "      <td>2.250</td>\n",
              "      <td>-81.380</td>\n",
              "      <td>-11.560</td>\n",
              "      <td>-96.010</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-116.840</td>\n",
              "      <td>-16.050</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317855</th>\n",
              "      <td>15892750</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-69.242</td>\n",
              "      <td>-8.337</td>\n",
              "      <td>-39.939</td>\n",
              "      <td>7.381</td>\n",
              "      <td>8.571</td>\n",
              "      <td>-86.680</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-80.758</td>\n",
              "      <td>-17.966</td>\n",
              "      <td>-99.500</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-109.700</td>\n",
              "      <td>-12.280</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317856</th>\n",
              "      <td>15892800</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-69.242</td>\n",
              "      <td>-8.337</td>\n",
              "      <td>-39.939</td>\n",
              "      <td>7.381</td>\n",
              "      <td>8.571</td>\n",
              "      <td>-86.680</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-80.758</td>\n",
              "      <td>-17.966</td>\n",
              "      <td>-99.500</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-109.700</td>\n",
              "      <td>-12.280</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317857</th>\n",
              "      <td>15892850</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-69.242</td>\n",
              "      <td>-8.337</td>\n",
              "      <td>-39.939</td>\n",
              "      <td>7.381</td>\n",
              "      <td>8.571</td>\n",
              "      <td>-86.680</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-80.758</td>\n",
              "      <td>-17.966</td>\n",
              "      <td>-99.500</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-109.700</td>\n",
              "      <td>-12.280</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317858</th>\n",
              "      <td>15892900</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-68.890</td>\n",
              "      <td>-8.481</td>\n",
              "      <td>-39.912</td>\n",
              "      <td>10.048</td>\n",
              "      <td>5.571</td>\n",
              "      <td>-86.680</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-80.750</td>\n",
              "      <td>-16.001</td>\n",
              "      <td>-99.500</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-109.700</td>\n",
              "      <td>-12.280</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317859</th>\n",
              "      <td>15892950</td>\n",
              "      <td>-71.053</td>\n",
              "      <td>42.367</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-68.890</td>\n",
              "      <td>-8.481</td>\n",
              "      <td>-39.912</td>\n",
              "      <td>10.048</td>\n",
              "      <td>5.571</td>\n",
              "      <td>-86.680</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-80.750</td>\n",
              "      <td>-16.001</td>\n",
              "      <td>-99.500</td>\n",
              "      <td>-10.510</td>\n",
              "      <td>-109.700</td>\n",
              "      <td>-12.280</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>317860 rows × 24 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-543783e6-b40e-442d-8a14-5ad99225f8ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-543783e6-b40e-442d-8a14-5ad99225f8ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-543783e6-b40e-442d-8a14-5ad99225f8ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class MyDataset:\n",
        "    def __init__(self, PARAMETERS:dict):\n",
        "        self.PARAMETERS = PARAMETERS\n",
        "        self.df = self.read_data()     \n",
        "        \n",
        "        if self.PARAMETERS[\"binary\"] == True:\n",
        "            self.convert_binary_label()\n",
        "            \n",
        "        self.lte_le = preprocessing.LabelEncoder()\n",
        "        self.nr_le = preprocessing.LabelEncoder()\n",
        "        self.all_le = preprocessing.LabelEncoder()\n",
        "        self.encode_label()\n",
        "            \n",
        "        if self.PARAMETERS[\"sample_rate\"] != 0:\n",
        "            self.sampled_df = self.sampling()\n",
        "    \n",
        "    \n",
        "    def read_data(self):\n",
        "        data_path = path.join(self.PARAMETERS[\"data_folder\"], self.PARAMETERS[\"file_name\"] + '.csv')\n",
        "        df = pd.read_csv(data_path, low_memory=False)\n",
        "        \n",
        "        # Drop the first several rows\n",
        "        df = df[3000:].reset_index(drop=True)                                   \n",
        "        \n",
        "        # Convert '2021-06-15 15:07:18.554116' to 1623769638554.116 (ms)\n",
        "        df['TIME_STAMP'] = pd.to_datetime(df['TIME_STAMP'])\n",
        "        df['OLD_TIME_STAMP'] = df['TIME_STAMP']\n",
        "        df[\"TIME_STAMP\"] = (df[\"TIME_STAMP\"] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1us')\n",
        "        df[\"TIME_STAMP\"] = (df[\"TIME_STAMP\"] - df[\"TIME_STAMP\"][0])/1000\n",
        "        \n",
        "        # Merge \"lte_ho_category\" and \"nr_ho_category\"\n",
        "        df[\"all_ho_category\"] = df[\"lte_ho_category\"] + \" \" + df[\"nr_ho_category\"]\n",
        "\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def convert_binary_label(self):\n",
        "        df = self.df\n",
        "        df.loc[~(self.df[\"lte_ho_category\"] == \"no_handover\"), \"lte_ho_category\"] = 1\n",
        "        df.loc[self.df[\"lte_ho_category\"] == \"no_handover\", \"lte_ho_category\"] = 0\n",
        "        df.loc[~(self.df[\"nr_ho_category\"] == \"no_handover\"), \"nr_ho_category\"] = 1\n",
        "        df.loc[self.df[\"nr_ho_category\"] == \"no_handover\", \"nr_ho_category\"] = 0\n",
        "        df.loc[~(self.df[\"all_ho_category\"] == \"no_handover no_handover\"), \"all_ho_category\"] = 1\n",
        "        df.loc[self.df[\"all_ho_category\"] == \"no_handover no_handover\", \"all_ho_category\"] = 0\n",
        "        self.df = df\n",
        "        \n",
        "        \n",
        "    def encode_label(self):\n",
        "        df = self.df\n",
        "        self.lte_le = preprocessing.LabelEncoder().fit(df[\"lte_ho_category\"])\n",
        "        df.loc[:, \"lte_ho_category\"] = self.lte_le.transform(df.loc[:, \"lte_ho_category\"])\n",
        "        self.nr_le = preprocessing.LabelEncoder().fit(df[\"nr_ho_category\"])\n",
        "        df.loc[:, \"nr_ho_category\"] = self.nr_le.transform(df.loc[:, \"nr_ho_category\"])\n",
        "        self.all_le = preprocessing.LabelEncoder().fit(df[\"all_ho_category\"])\n",
        "        df.loc[:, \"all_ho_category\"] = self.all_le.transform(df.loc[:, \"all_ho_category\"])\n",
        "        self.df = df\n",
        "\n",
        "\n",
        "    def sampling(self):\n",
        "        df = self.df\n",
        "        sample_rate = self.PARAMETERS[\"sample_rate\"]  # default 50 ms\n",
        "        bins = range(int(df['TIME_STAMP'].min()), int(df['TIME_STAMP'].max())+1, sample_rate)\n",
        "        df['r_time'] = pd.cut(df['TIME_STAMP'], bins=bins, labels=bins[:-1], include_lowest=True)\n",
        "        sampled_df = df.groupby('r_time').agg(lon=('lon', np.mean), \n",
        "                                              lat=('lat', np.mean),\n",
        "                                              speed=('speed', np.mean),\n",
        "                                              lte_serv_rsrp=('lte_serv_rsrp', np.mean),\n",
        "                                              lte_serv_rsrq=('lte_serv_rsrq', np.mean),\n",
        "                                              lte_serv_rssi=('lte_serv_rssi', np.mean),\n",
        "                                              lte_cqi0=('lte_cqi0', np.mean),\n",
        "                                              lte_cqi1=('lte_cqi1', np.mean),\n",
        "                                              nr_serv_rsrp=('nr_serv_rsrp', np.mean),\n",
        "                                              nr_serv_rsrq=('nr_serv_rsrq', np.mean),\n",
        "                                              nr_serv_sinr=('nr_serv_sinr', np.mean),\n",
        "                                              nr_cqi=('nr_cqi', np.mean),\n",
        "                                              dl_tput=('dl_tput', np.mean),\n",
        "                                              ul_tput=('ul_tput', np.mean),\n",
        "                                              lte_nbr_rsrp=('lte_nbr_rsrp', np.mean),\n",
        "                                              lte_nbr_rsrq=('lte_nbr_rsrq', np.mean),\n",
        "                                              irat_rsrp=('irat_rsrp', np.mean),\n",
        "                                              irat_rsrq=('irat_rsrq', np.mean),\n",
        "                                              nr_nbr_rsrp=('nr_nbr_rsrp', np.mean),\n",
        "                                              nr_nbr_rsrq=('nr_nbr_rsrq', np.mean),\n",
        "                                              lte_ho_category=('lte_ho_category', np.max),\n",
        "                                              nr_ho_category=('nr_ho_category', np.max),\n",
        "                                              all_ho_category=('all_ho_category', np.max),) \n",
        "        # Padding 4*sample_rate=200 ms\n",
        "        for col in sampled_df:\n",
        "            if col ==\"r_time\":\n",
        "                continue\n",
        "            else:\n",
        "                sampled_df[col].fillna(method='ffill', limit=4, inplace=True)\n",
        "                \n",
        "        sampled_df.reset_index(level=0, inplace=True)\n",
        "        return sampled_df\n",
        "\n",
        "\n",
        "    def get_df(self):\n",
        "        return self.df\n",
        "\n",
        "\n",
        "    def get_sampled_df(self):\n",
        "        return self.sampled_df\n",
        "\n",
        "\n",
        "####################################\n",
        "### Read and sample the database\n",
        "####################################\n",
        "Dataset = MyDataset(PARAMETERS)\n",
        "df = Dataset.get_df()\n",
        "print(df.columns)\n",
        "\n",
        "sampled_df = Dataset.get_sampled_df()\n",
        "display(sampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "maritime-northwest",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maritime-northwest",
        "outputId": "b181fc57-bc94-42c7-d8f0-ec7ceda89550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing train set: \n",
            "\n",
            "seq_ho_arr shape:  (2271, 40)\n",
            "seq_neg_arr shape:  (7554, 40)\n",
            "data_seq_point_ind shape:  (9825, 40)\n",
            "Processing:  0 / 9825\n",
            "Processing:  500 / 9825\n",
            "Processing:  1000 / 9825\n",
            "Processing:  1500 / 9825\n",
            "Processing:  2000 / 9825\n",
            "Processing:  2500 / 9825\n",
            "Processing:  3000 / 9825\n",
            "Processing:  3500 / 9825\n",
            "Processing:  4000 / 9825\n",
            "Processing:  4500 / 9825\n",
            "Processing:  5000 / 9825\n",
            "Processing:  5500 / 9825\n",
            "Processing:  6000 / 9825\n",
            "Processing:  6500 / 9825\n",
            "Processing:  7000 / 9825\n",
            "Processing:  7500 / 9825\n",
            "Processing:  8000 / 9825\n",
            "Processing:  8500 / 9825\n",
            "Processing:  9000 / 9825\n",
            "Processing:  9500 / 9825\n",
            "\n",
            "X_data_seq shape:  (9825, 40, 17)  y_data_seq shape:  (9825, 40)\n",
            "\n",
            "Processing test set: \n",
            "\n",
            "seq_ho_arr shape:  (488, 40)\n",
            "seq_all_arr shape:  (10000, 40)\n",
            "data_seq_point_ind shape:  (10488, 40)\n",
            "Processing:  0 / 10488\n",
            "Processing:  500 / 10488\n",
            "Processing:  1000 / 10488\n",
            "Processing:  1500 / 10488\n",
            "Processing:  2000 / 10488\n",
            "Processing:  2500 / 10488\n",
            "Processing:  3000 / 10488\n",
            "Processing:  3500 / 10488\n",
            "Processing:  4000 / 10488\n",
            "Processing:  4500 / 10488\n",
            "Processing:  5000 / 10488\n",
            "Processing:  5500 / 10488\n",
            "Processing:  6000 / 10488\n",
            "Processing:  6500 / 10488\n",
            "Processing:  7000 / 10488\n",
            "Processing:  7500 / 10488\n",
            "Processing:  8000 / 10488\n",
            "Processing:  8500 / 10488\n",
            "Processing:  9000 / 10488\n",
            "Processing:  9500 / 10488\n",
            "Processing:  10000 / 10488\n",
            "\n",
            "X_data_seq shape:  (10488, 40, 17)  y_data_seq shape:  (10488, 40)\n",
            "\n",
            "X_train_seq shape:  (9825, 40, 17)  y_train_seq shape:  (9825, 40)\n",
            "X_test_seq shape:  (10488, 40, 17)  y_train_seq shape:  (10488, 40)\n"
          ]
        }
      ],
      "source": [
        "def sample_ho_seq(df, PARAMETERS):\n",
        "    ho_locs = np.where(df[PARAMETERS[\"label\"]] == 1)[0]\n",
        "    nan_locs = np.where(df[PARAMETERS[\"label\"]] != df[PARAMETERS[\"label\"]])[0]\n",
        "    \n",
        "    ### generate ho sequences from df\n",
        "    # Total 40 time slots in each sequence, each HO contributes 5 sequences \n",
        "    seq_ho_list = []\n",
        "    previous_num_points = np.random.randint(40, size=(len(ho_locs), 10))  \n",
        "    for i, ho_loc in enumerate(ho_locs):\n",
        "        for pre_num in previous_num_points[i]:\n",
        "            pre_loc = ho_loc-pre_num\n",
        "            beh_loc = pre_loc+40\n",
        "            seq_ho_list.append(range(pre_loc, beh_loc))\n",
        "\n",
        "    # check no Nan value in seq\n",
        "    seq_save = []\n",
        "    for i, seq in enumerate(seq_ho_list):\n",
        "        seq_min = seq[0]\n",
        "        seq_max = seq[-1]\n",
        "        if len(np.where((nan_locs>=seq_min) & (nan_locs<=seq_max))[0])==0 and \\\n",
        "          seq_max<len(df):\n",
        "            seq_save.append(i)\n",
        "\n",
        "    seq_ho_arr = np.array(seq_ho_list)[seq_save]\n",
        "    print(\"\\nseq_ho_arr shape: \", seq_ho_arr.shape)\n",
        "    return seq_ho_arr\n",
        "\n",
        "\n",
        "def sample_neg_seq(df, PARAMETERS):\n",
        "    ho_locs = np.where(df[PARAMETERS[\"label\"]] == 1)[0]\n",
        "    nan_locs = np.where(df[PARAMETERS[\"label\"]] != df[PARAMETERS[\"label\"]])[0]\n",
        "\n",
        "    ### generate neg sequences from df\n",
        "    seq_neg_list = []\n",
        "    rand_arr = np.random.randint(len(df), size=PARAMETERS[\"neg_size\"]) \n",
        "    for pre_loc in rand_arr:\n",
        "        beh_loc = pre_loc+40                                                                                 \n",
        "        seq_neg_list.append(range(pre_loc, beh_loc))\n",
        "\n",
        "    # check no Nan and ho in seq\n",
        "    seq_save = []\n",
        "    for i, seq in enumerate(seq_neg_list):\n",
        "        seq_min = seq[0]\n",
        "        seq_max = seq[-1]\n",
        "        if len(np.where((nan_locs>=seq_min) & (nan_locs<=seq_max))[0])==0 and \\\n",
        "          len(np.where((ho_locs>=seq_min) & (ho_locs<=seq_max))[0])==0 and \\\n",
        "          seq_max<len(df):\n",
        "                seq_save.append(i)    \n",
        "\n",
        "    seq_neg_arr = np.array(seq_neg_list)[seq_save]\n",
        "    print(\"seq_neg_arr shape: \", seq_neg_arr.shape)\n",
        "    return seq_neg_arr\n",
        "\n",
        "\n",
        "def sample_seq(df, PARAMETERS):\n",
        "    # Input: df should start from index 0\n",
        "\n",
        "    ### generate data sequences \n",
        "    seq_ho_arr = sample_ho_seq(df, PARAMETERS)\n",
        "    seq_neg_arr = sample_neg_seq(df, PARAMETERS)\n",
        "    data_seq_point_ind = np.concatenate((seq_ho_arr, seq_neg_arr), axis=0)\n",
        "    print(\"data_seq_point_ind shape: \", data_seq_point_ind.shape)\n",
        "\n",
        "    ### retrieve the features\n",
        "    data_seq = []\n",
        "    for i, seq in enumerate(data_seq_point_ind):\n",
        "        if i%500==0:\n",
        "            print(\"Processing: \", i,\"/\",len(data_seq_point_ind))\n",
        "        data_seq.append(np.array(df.loc[seq, PARAMETERS[\"feature_list\"]+[PARAMETERS[\"label\"]]]))\n",
        "    X_data_seq, y_data_seq = np.split(np.array(data_seq), [len(PARAMETERS[\"feature_list\"])], axis=2)\n",
        "    y_data_seq = y_data_seq.squeeze()\n",
        "    print(\"\\nX_data_seq shape: \", X_data_seq.shape, \" y_data_seq shape: \", y_data_seq.shape)\n",
        "\n",
        "    ### split the dataset into train and test\n",
        "    X_data_seq_ind = np.arange(len(X_data_seq))\n",
        "    np.random.shuffle(X_data_seq_ind)\n",
        "\n",
        "    if PARAMETERS[\"split_mode\"] == \"all_neg_sampling\":\n",
        "        num_train_samples = int(PARAMETERS[\"train_ratio\"]*len(X_data_seq_ind))\n",
        "    elif PARAMETERS[\"split_mode\"] == \"train_neg_sampling\":\n",
        "        num_train_samples = int(len(X_data_seq_ind))\n",
        "    X_train_seq = X_data_seq[X_data_seq_ind][:num_train_samples]\n",
        "    y_train_seq = y_data_seq[X_data_seq_ind][:num_train_samples]\n",
        "    X_test_seq = X_data_seq[X_data_seq_ind][num_train_samples:]\n",
        "    y_test_seq = y_data_seq[X_data_seq_ind][num_train_samples:]\n",
        "    return X_train_seq, y_train_seq, X_test_seq, y_test_seq\n",
        "\n",
        "\n",
        "def convert_df_to_seq(df, PARAMETERS):\n",
        "    df = df.reset_index(drop=True)\n",
        "    nan_loc = np.where(df[PARAMETERS[\"label\"]] != df[PARAMETERS[\"label\"]])[0]\n",
        "\n",
        "    ### generate ind sequences \n",
        "    seq_ho_arr = sample_ho_seq(df, PARAMETERS)\n",
        "\n",
        "    seq_all_list = []\n",
        "    for ind in range(len(df)-40):                                                                             \n",
        "        if len(np.where((nan_loc>=ind) & (nan_loc<=ind+40))[0])==0:\n",
        "            seq_all_list.append(range(ind, ind+40))\n",
        "    random_list = random.sample(range(len(seq_all_list)), 10000)\n",
        "    seq_all_arr = np.array(seq_all_list)[random_list]\n",
        "    print(\"seq_all_arr shape: \", seq_all_arr.shape)\n",
        "\n",
        "    data_seq_point_ind = np.concatenate((seq_ho_arr, seq_all_arr), axis=0)\n",
        "    print(\"data_seq_point_ind shape: \", data_seq_point_ind.shape)\n",
        "\n",
        "\n",
        "    ### retrieve the features\n",
        "    data_seq = []\n",
        "    for i, seq in enumerate(data_seq_point_ind):\n",
        "        if i%500==0:\n",
        "            print(\"Processing: \", i,\"/\",len(data_seq_point_ind))\n",
        "        data_seq.append(np.array(df.loc[seq, PARAMETERS[\"feature_list\"]+[PARAMETERS[\"label\"]]]))\n",
        "    X_data_seq, y_data_seq = np.split(np.array(data_seq), [len(PARAMETERS[\"feature_list\"])], axis=2)\n",
        "    y_data_seq = y_data_seq.squeeze()\n",
        "    print(\"\\nX_data_seq shape: \", X_data_seq.shape, \" y_data_seq shape: \", y_data_seq.shape)\n",
        "\n",
        "    return X_data_seq, y_data_seq\n",
        "\n",
        "\n",
        "####################################\n",
        "### Generate data sequences for train and test \n",
        "####################################\n",
        "if PARAMETERS[\"split_mode\"] == \"all_neg_sampling\":\n",
        "    # Do the negative sampling on the whole dataset\n",
        "    # Split the sampled sequences for train and test\n",
        "    X_train_seq, y_train_seq, X_test_seq, y_test_seq = sample_seq(sampled_df, PARAMETERS)\n",
        "\n",
        "\n",
        "if PARAMETERS[\"split_mode\"] == \"train_neg_sampling\":\n",
        "    # Split the dataset into training and test part\n",
        "    # Do the negative sampling on the traing dataset, \n",
        "    # Use the sampled sequences for training, and the whole test dataset for test \n",
        "    num_train_samples = int(PARAMETERS[\"train_ratio\"]*len(sampled_df))\n",
        "    train_df = sampled_df[:num_train_samples]\n",
        "    test_df = sampled_df[num_train_samples:]\n",
        "    print(\"\\nProcessing train set: \")\n",
        "    X_train_seq, y_train_seq, _, _ = sample_seq(train_df, PARAMETERS)\n",
        "    print(\"\\nProcessing test set: \")\n",
        "    X_test_seq, y_test_seq = convert_df_to_seq(test_df, PARAMETERS)\n",
        "\n",
        "\n",
        "print(\"\\nX_train_seq shape: \", X_train_seq.shape, \" y_train_seq shape: \", y_train_seq.shape)\n",
        "print(\"X_test_seq shape: \", X_test_seq.shape, \" y_train_seq shape: \", y_test_seq.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: Define model and do training\n",
        "\n"
      ],
      "metadata": {
        "id": "mr_8MtKYDbJf"
      },
      "id": "mr_8MtKYDbJf"
    },
    {
      "cell_type": "code",
      "source": [
        "# further split the traing data for model \n",
        "X_train = np.hsplit(X_train_seq, 2)[0]\n",
        "y_train = np.hsplit(y_train_seq, 2)[1].max(axis=1)\n",
        "print(\"X_train shape: \", X_train.shape, \" y_train shape: \", y_train.shape)\n",
        "\n",
        "# further split the test data for model \n",
        "X_test  = np.hsplit(X_test_seq, 2)[0]\n",
        "y_test = np.hsplit(y_test_seq, 2)[1].max(axis=1)\n",
        "print(\"X_test shape: \", X_test.shape, \" y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h35-HmcxDgdq",
        "outputId": "592396e7-6e3b-4711-f6b3-cba65e512596"
      },
      "id": "h35-HmcxDgdq",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  (9825, 20, 17)  y_train shape:  (9825,)\n",
            "X_test shape:  (10488, 20, 17)  y_test shape:  (10488,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "batch_size = PARAMETERS[\"batch_size\"]\n",
        "epochs = PARAMETERS[\"epochs\"]\n",
        "latent_dim = PARAMETERS[\"latent_dim\"]\n",
        "\n",
        "\n",
        "####################################\n",
        "# Stacked LSTM Architecture\n",
        "####################################\n",
        "# Stacked LSTM\n",
        "stacked_LSTM_model = tf.keras.Sequential()\n",
        "stacked_LSTM_model.add(LSTM(latent_dim, return_sequences=True, input_shape=(20, len(PARAMETERS[\"feature_list\"]))))   # returns a sequence of vectors of dimension 32\n",
        "stacked_LSTM_model.add(LSTM(latent_dim, return_sequences=True))                                                      # returns a sequence of vectors of dimension 32\n",
        "stacked_LSTM_model.add(LSTM(latent_dim))                                                                             # return a single vector of dimension 32\n",
        "stacked_LSTM_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "# Model compile\n",
        "model = stacked_LSTM_model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# Training\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
        "model.fit(X_train,\n",
        "          y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks=[callback],\n",
        "          validation_split=0.2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOwZvQIxG3lt",
        "outputId": "9dd4e847-948d-4fc6-cc19-8702a7a66fd2"
      },
      "id": "tOwZvQIxG3lt",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "123/123 [==============================] - 18s 76ms/step - loss: 0.3779 - accuracy: 0.8692 - val_loss: 0.3309 - val_accuracy: 0.8743\n",
            "Epoch 2/200\n",
            "123/123 [==============================] - 6s 51ms/step - loss: 0.3061 - accuracy: 0.8781 - val_loss: 0.2776 - val_accuracy: 0.8779\n",
            "Epoch 3/200\n",
            "123/123 [==============================] - 6s 48ms/step - loss: 0.2807 - accuracy: 0.8854 - val_loss: 0.2688 - val_accuracy: 0.8875\n",
            "Epoch 4/200\n",
            "123/123 [==============================] - 4s 31ms/step - loss: 0.2759 - accuracy: 0.8920 - val_loss: 0.2546 - val_accuracy: 0.8936\n",
            "Epoch 5/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2644 - accuracy: 0.8947 - val_loss: 0.2501 - val_accuracy: 0.9079\n",
            "Epoch 6/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2550 - accuracy: 0.9048 - val_loss: 0.2322 - val_accuracy: 0.9125\n",
            "Epoch 7/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2601 - accuracy: 0.8986 - val_loss: 0.2502 - val_accuracy: 0.9018\n",
            "Epoch 8/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2453 - accuracy: 0.9075 - val_loss: 0.2336 - val_accuracy: 0.9094\n",
            "Epoch 9/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2390 - accuracy: 0.9127 - val_loss: 0.2388 - val_accuracy: 0.9130\n",
            "Epoch 10/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2377 - accuracy: 0.9127 - val_loss: 0.2321 - val_accuracy: 0.9115\n",
            "Epoch 11/200\n",
            "123/123 [==============================] - 3s 25ms/step - loss: 0.2372 - accuracy: 0.9104 - val_loss: 0.2465 - val_accuracy: 0.9104\n",
            "Epoch 12/200\n",
            "123/123 [==============================] - 3s 25ms/step - loss: 0.2343 - accuracy: 0.9117 - val_loss: 0.2409 - val_accuracy: 0.9079\n",
            "Epoch 13/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2278 - accuracy: 0.9159 - val_loss: 0.2187 - val_accuracy: 0.9201\n",
            "Epoch 14/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2269 - accuracy: 0.9164 - val_loss: 0.2175 - val_accuracy: 0.9191\n",
            "Epoch 15/200\n",
            "123/123 [==============================] - 3s 25ms/step - loss: 0.2228 - accuracy: 0.9176 - val_loss: 0.2375 - val_accuracy: 0.9150\n",
            "Epoch 16/200\n",
            "123/123 [==============================] - 3s 25ms/step - loss: 0.2306 - accuracy: 0.9128 - val_loss: 0.2221 - val_accuracy: 0.9211\n",
            "Epoch 17/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2206 - accuracy: 0.9170 - val_loss: 0.2229 - val_accuracy: 0.9211\n",
            "Epoch 18/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2203 - accuracy: 0.9146 - val_loss: 0.2209 - val_accuracy: 0.9211\n",
            "Epoch 19/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2214 - accuracy: 0.9169 - val_loss: 0.2497 - val_accuracy: 0.9074\n",
            "Epoch 20/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2197 - accuracy: 0.9187 - val_loss: 0.2154 - val_accuracy: 0.9201\n",
            "Epoch 21/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2150 - accuracy: 0.9211 - val_loss: 0.2160 - val_accuracy: 0.9226\n",
            "Epoch 22/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2099 - accuracy: 0.9209 - val_loss: 0.2416 - val_accuracy: 0.9094\n",
            "Epoch 23/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2109 - accuracy: 0.9202 - val_loss: 0.2079 - val_accuracy: 0.9267\n",
            "Epoch 24/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2178 - accuracy: 0.9191 - val_loss: 0.2053 - val_accuracy: 0.9211\n",
            "Epoch 25/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2090 - accuracy: 0.9212 - val_loss: 0.2373 - val_accuracy: 0.9125\n",
            "Epoch 26/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.2118 - accuracy: 0.9229 - val_loss: 0.2136 - val_accuracy: 0.9257\n",
            "Epoch 27/200\n",
            "123/123 [==============================] - 4s 34ms/step - loss: 0.2100 - accuracy: 0.9226 - val_loss: 0.2038 - val_accuracy: 0.9308\n",
            "Epoch 28/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2051 - accuracy: 0.9254 - val_loss: 0.2097 - val_accuracy: 0.9221\n",
            "Epoch 29/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2085 - accuracy: 0.9209 - val_loss: 0.2132 - val_accuracy: 0.9181\n",
            "Epoch 30/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2057 - accuracy: 0.9243 - val_loss: 0.2184 - val_accuracy: 0.9191\n",
            "Epoch 31/200\n",
            "123/123 [==============================] - 5s 37ms/step - loss: 0.2078 - accuracy: 0.9226 - val_loss: 0.2056 - val_accuracy: 0.9237\n",
            "Epoch 32/200\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.2101 - accuracy: 0.9226 - val_loss: 0.2249 - val_accuracy: 0.9130\n",
            "Epoch 33/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2055 - accuracy: 0.9246 - val_loss: 0.1913 - val_accuracy: 0.9272\n",
            "Epoch 34/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2012 - accuracy: 0.9256 - val_loss: 0.2075 - val_accuracy: 0.9267\n",
            "Epoch 35/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.2070 - accuracy: 0.9233 - val_loss: 0.1952 - val_accuracy: 0.9221\n",
            "Epoch 36/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1950 - accuracy: 0.9276 - val_loss: 0.1926 - val_accuracy: 0.9298\n",
            "Epoch 37/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1954 - accuracy: 0.9280 - val_loss: 0.2023 - val_accuracy: 0.9272\n",
            "Epoch 38/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1936 - accuracy: 0.9298 - val_loss: 0.1890 - val_accuracy: 0.9323\n",
            "Epoch 39/200\n",
            "123/123 [==============================] - 6s 45ms/step - loss: 0.1899 - accuracy: 0.9307 - val_loss: 0.1867 - val_accuracy: 0.9344\n",
            "Epoch 40/200\n",
            "123/123 [==============================] - 4s 35ms/step - loss: 0.1947 - accuracy: 0.9266 - val_loss: 0.2005 - val_accuracy: 0.9333\n",
            "Epoch 41/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1966 - accuracy: 0.9298 - val_loss: 0.1924 - val_accuracy: 0.9328\n",
            "Epoch 42/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1921 - accuracy: 0.9285 - val_loss: 0.1841 - val_accuracy: 0.9338\n",
            "Epoch 43/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1884 - accuracy: 0.9293 - val_loss: 0.2064 - val_accuracy: 0.9196\n",
            "Epoch 44/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1897 - accuracy: 0.9284 - val_loss: 0.2072 - val_accuracy: 0.9232\n",
            "Epoch 45/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1942 - accuracy: 0.9299 - val_loss: 0.1952 - val_accuracy: 0.9318\n",
            "Epoch 46/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1903 - accuracy: 0.9294 - val_loss: 0.1795 - val_accuracy: 0.9344\n",
            "Epoch 47/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1818 - accuracy: 0.9316 - val_loss: 0.1854 - val_accuracy: 0.9308\n",
            "Epoch 48/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1776 - accuracy: 0.9324 - val_loss: 0.2033 - val_accuracy: 0.9267\n",
            "Epoch 49/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1888 - accuracy: 0.9270 - val_loss: 0.1879 - val_accuracy: 0.9344\n",
            "Epoch 50/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1773 - accuracy: 0.9323 - val_loss: 0.1846 - val_accuracy: 0.9338\n",
            "Epoch 51/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1794 - accuracy: 0.9337 - val_loss: 0.2010 - val_accuracy: 0.9196\n",
            "Epoch 52/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1859 - accuracy: 0.9303 - val_loss: 0.1897 - val_accuracy: 0.9293\n",
            "Epoch 53/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1847 - accuracy: 0.9336 - val_loss: 0.1927 - val_accuracy: 0.9323\n",
            "Epoch 54/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1896 - accuracy: 0.9307 - val_loss: 0.1996 - val_accuracy: 0.9201\n",
            "Epoch 55/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1813 - accuracy: 0.9327 - val_loss: 0.1962 - val_accuracy: 0.9318\n",
            "Epoch 56/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1774 - accuracy: 0.9318 - val_loss: 0.1853 - val_accuracy: 0.9303\n",
            "Epoch 57/200\n",
            "123/123 [==============================] - 4s 28ms/step - loss: 0.1730 - accuracy: 0.9346 - val_loss: 0.2019 - val_accuracy: 0.9288\n",
            "Epoch 58/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1806 - accuracy: 0.9323 - val_loss: 0.1934 - val_accuracy: 0.9303\n",
            "Epoch 59/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1751 - accuracy: 0.9335 - val_loss: 0.1777 - val_accuracy: 0.9308\n",
            "Epoch 60/200\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.1771 - accuracy: 0.9302 - val_loss: 0.1808 - val_accuracy: 0.9313\n",
            "Epoch 61/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1771 - accuracy: 0.9322 - val_loss: 0.1789 - val_accuracy: 0.9354\n",
            "Epoch 62/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1695 - accuracy: 0.9351 - val_loss: 0.1738 - val_accuracy: 0.9344\n",
            "Epoch 63/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1617 - accuracy: 0.9374 - val_loss: 0.1712 - val_accuracy: 0.9364\n",
            "Epoch 64/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1622 - accuracy: 0.9379 - val_loss: 0.1947 - val_accuracy: 0.9252\n",
            "Epoch 65/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1636 - accuracy: 0.9366 - val_loss: 0.1995 - val_accuracy: 0.9237\n",
            "Epoch 66/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1609 - accuracy: 0.9373 - val_loss: 0.1707 - val_accuracy: 0.9282\n",
            "Epoch 67/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1703 - accuracy: 0.9336 - val_loss: 0.2048 - val_accuracy: 0.9196\n",
            "Epoch 68/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1550 - accuracy: 0.9378 - val_loss: 0.1858 - val_accuracy: 0.9298\n",
            "Epoch 69/200\n",
            "123/123 [==============================] - 4s 28ms/step - loss: 0.1500 - accuracy: 0.9416 - val_loss: 0.1911 - val_accuracy: 0.9272\n",
            "Epoch 70/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1604 - accuracy: 0.9388 - val_loss: 0.2061 - val_accuracy: 0.9206\n",
            "Epoch 71/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1656 - accuracy: 0.9373 - val_loss: 0.1883 - val_accuracy: 0.9333\n",
            "Epoch 72/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1648 - accuracy: 0.9379 - val_loss: 0.1650 - val_accuracy: 0.9364\n",
            "Epoch 73/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1436 - accuracy: 0.9441 - val_loss: 0.1849 - val_accuracy: 0.9272\n",
            "Epoch 74/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1509 - accuracy: 0.9436 - val_loss: 0.1612 - val_accuracy: 0.9435\n",
            "Epoch 75/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1440 - accuracy: 0.9424 - val_loss: 0.2243 - val_accuracy: 0.9181\n",
            "Epoch 76/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1603 - accuracy: 0.9429 - val_loss: 0.1977 - val_accuracy: 0.9303\n",
            "Epoch 77/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1489 - accuracy: 0.9429 - val_loss: 0.1759 - val_accuracy: 0.9379\n",
            "Epoch 78/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1404 - accuracy: 0.9478 - val_loss: 0.1611 - val_accuracy: 0.9399\n",
            "Epoch 79/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1514 - accuracy: 0.9411 - val_loss: 0.1773 - val_accuracy: 0.9354\n",
            "Epoch 80/200\n",
            "123/123 [==============================] - 3s 26ms/step - loss: 0.1490 - accuracy: 0.9457 - val_loss: 0.1559 - val_accuracy: 0.9425\n",
            "Epoch 81/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1446 - accuracy: 0.9463 - val_loss: 0.1623 - val_accuracy: 0.9394\n",
            "Epoch 82/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1453 - accuracy: 0.9449 - val_loss: 0.1604 - val_accuracy: 0.9374\n",
            "Epoch 83/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1430 - accuracy: 0.9459 - val_loss: 0.1611 - val_accuracy: 0.9405\n",
            "Epoch 84/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1326 - accuracy: 0.9496 - val_loss: 0.1613 - val_accuracy: 0.9450\n",
            "Epoch 85/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1474 - accuracy: 0.9444 - val_loss: 0.1828 - val_accuracy: 0.9323\n",
            "Epoch 86/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1344 - accuracy: 0.9533 - val_loss: 0.1504 - val_accuracy: 0.9440\n",
            "Epoch 87/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1346 - accuracy: 0.9505 - val_loss: 0.1477 - val_accuracy: 0.9405\n",
            "Epoch 88/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1326 - accuracy: 0.9520 - val_loss: 0.1565 - val_accuracy: 0.9445\n",
            "Epoch 89/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1281 - accuracy: 0.9518 - val_loss: 0.1488 - val_accuracy: 0.9471\n",
            "Epoch 90/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1397 - accuracy: 0.9481 - val_loss: 0.1777 - val_accuracy: 0.9394\n",
            "Epoch 91/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1356 - accuracy: 0.9506 - val_loss: 0.1669 - val_accuracy: 0.9410\n",
            "Epoch 92/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1358 - accuracy: 0.9504 - val_loss: 0.1568 - val_accuracy: 0.9445\n",
            "Epoch 93/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1266 - accuracy: 0.9536 - val_loss: 0.1607 - val_accuracy: 0.9435\n",
            "Epoch 94/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1210 - accuracy: 0.9559 - val_loss: 0.1630 - val_accuracy: 0.9440\n",
            "Epoch 95/200\n",
            "123/123 [==============================] - 4s 35ms/step - loss: 0.1286 - accuracy: 0.9545 - val_loss: 0.1671 - val_accuracy: 0.9389\n",
            "Epoch 96/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1282 - accuracy: 0.9538 - val_loss: 0.1903 - val_accuracy: 0.9252\n",
            "Epoch 97/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1353 - accuracy: 0.9489 - val_loss: 0.1636 - val_accuracy: 0.9410\n",
            "Epoch 98/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1222 - accuracy: 0.9559 - val_loss: 0.1634 - val_accuracy: 0.9410\n",
            "Epoch 99/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1263 - accuracy: 0.9525 - val_loss: 0.1741 - val_accuracy: 0.9374\n",
            "Epoch 100/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1317 - accuracy: 0.9514 - val_loss: 0.2023 - val_accuracy: 0.9272\n",
            "Epoch 101/200\n",
            "123/123 [==============================] - 3s 27ms/step - loss: 0.1339 - accuracy: 0.9513 - val_loss: 0.1588 - val_accuracy: 0.9450\n",
            "Epoch 102/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1225 - accuracy: 0.9553 - val_loss: 0.1657 - val_accuracy: 0.9415\n",
            "Epoch 103/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1280 - accuracy: 0.9542 - val_loss: 0.1667 - val_accuracy: 0.9389\n",
            "Epoch 104/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1280 - accuracy: 0.9522 - val_loss: 0.1587 - val_accuracy: 0.9399\n",
            "Epoch 105/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1283 - accuracy: 0.9546 - val_loss: 0.2024 - val_accuracy: 0.9313\n",
            "Epoch 106/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1342 - accuracy: 0.9492 - val_loss: 0.1669 - val_accuracy: 0.9333\n",
            "Epoch 107/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1098 - accuracy: 0.9611 - val_loss: 0.1491 - val_accuracy: 0.9476\n",
            "Epoch 108/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1041 - accuracy: 0.9623 - val_loss: 0.1455 - val_accuracy: 0.9476\n",
            "Epoch 109/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1230 - accuracy: 0.9556 - val_loss: 0.1570 - val_accuracy: 0.9496\n",
            "Epoch 110/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1125 - accuracy: 0.9601 - val_loss: 0.1575 - val_accuracy: 0.9415\n",
            "Epoch 111/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1365 - accuracy: 0.9508 - val_loss: 0.1630 - val_accuracy: 0.9420\n",
            "Epoch 112/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1288 - accuracy: 0.9517 - val_loss: 0.1778 - val_accuracy: 0.9308\n",
            "Epoch 113/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1076 - accuracy: 0.9602 - val_loss: 0.1525 - val_accuracy: 0.9491\n",
            "Epoch 114/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1093 - accuracy: 0.9602 - val_loss: 0.1666 - val_accuracy: 0.9461\n",
            "Epoch 115/200\n",
            "123/123 [==============================] - 3s 28ms/step - loss: 0.1206 - accuracy: 0.9557 - val_loss: 0.1698 - val_accuracy: 0.9399\n",
            "Epoch 116/200\n",
            "123/123 [==============================] - 4s 28ms/step - loss: 0.1070 - accuracy: 0.9621 - val_loss: 0.1545 - val_accuracy: 0.9430\n",
            "Epoch 117/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1151 - accuracy: 0.9569 - val_loss: 0.1564 - val_accuracy: 0.9415\n",
            "Epoch 118/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.0950 - accuracy: 0.9662 - val_loss: 0.1495 - val_accuracy: 0.9496\n",
            "Epoch 119/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1002 - accuracy: 0.9640 - val_loss: 0.1494 - val_accuracy: 0.9491\n",
            "Epoch 120/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1042 - accuracy: 0.9632 - val_loss: 0.1451 - val_accuracy: 0.9471\n",
            "Epoch 121/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1280 - accuracy: 0.9522 - val_loss: 0.1625 - val_accuracy: 0.9389\n",
            "Epoch 122/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1167 - accuracy: 0.9551 - val_loss: 0.1505 - val_accuracy: 0.9430\n",
            "Epoch 123/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1082 - accuracy: 0.9598 - val_loss: 0.1642 - val_accuracy: 0.9425\n",
            "Epoch 124/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.0991 - accuracy: 0.9653 - val_loss: 0.1585 - val_accuracy: 0.9455\n",
            "Epoch 125/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1033 - accuracy: 0.9621 - val_loss: 0.1551 - val_accuracy: 0.9466\n",
            "Epoch 126/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.0980 - accuracy: 0.9632 - val_loss: 0.1689 - val_accuracy: 0.9399\n",
            "Epoch 127/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1176 - accuracy: 0.9607 - val_loss: 0.1756 - val_accuracy: 0.9405\n",
            "Epoch 128/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1234 - accuracy: 0.9547 - val_loss: 0.1632 - val_accuracy: 0.9430\n",
            "Epoch 129/200\n",
            "123/123 [==============================] - 4s 36ms/step - loss: 0.0980 - accuracy: 0.9648 - val_loss: 0.1472 - val_accuracy: 0.9476\n",
            "Epoch 130/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1062 - accuracy: 0.9631 - val_loss: 0.1416 - val_accuracy: 0.9486\n",
            "Epoch 131/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1169 - accuracy: 0.9589 - val_loss: 0.1921 - val_accuracy: 0.9354\n",
            "Epoch 132/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1497 - accuracy: 0.9421 - val_loss: 0.1976 - val_accuracy: 0.9313\n",
            "Epoch 133/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1302 - accuracy: 0.9509 - val_loss: 0.1590 - val_accuracy: 0.9374\n",
            "Epoch 134/200\n",
            "123/123 [==============================] - 4s 30ms/step - loss: 0.1283 - accuracy: 0.9517 - val_loss: 0.1782 - val_accuracy: 0.9354\n",
            "Epoch 135/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1285 - accuracy: 0.9537 - val_loss: 0.1732 - val_accuracy: 0.9420\n",
            "Epoch 136/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1265 - accuracy: 0.9520 - val_loss: 0.1601 - val_accuracy: 0.9405\n",
            "Epoch 137/200\n",
            "123/123 [==============================] - 4s 29ms/step - loss: 0.1351 - accuracy: 0.9511 - val_loss: 0.1726 - val_accuracy: 0.9338\n",
            "Epoch 138/200\n",
            "123/123 [==============================] - 4s 30ms/step - loss: 0.1412 - accuracy: 0.9477 - val_loss: 0.1753 - val_accuracy: 0.9374\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f483b5ce090>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the trained model"
      ],
      "metadata": {
        "id": "-At9IgT6QDxf"
      },
      "id": "-At9IgT6QDxf"
    },
    {
      "cell_type": "code",
      "source": [
        "##########################\n",
        "### Evaluation\n",
        "##########################\n",
        "y_pred = model.predict([X_test])\n",
        "y_pred = y_pred.reshape(-1) // 0.5\n",
        "y_true = y_test \n",
        "\n",
        "bi_f1 =  f1_score(y_true, y_pred) \n",
        "bi_recall = recall_score(y_true, y_pred)\n",
        "bi_precision = precision_score(y_true, y_pred)\n",
        "bi_acc = accuracy_score(y_true, y_pred)\n",
        "print(\"Binary classification: F1-score: {:.3} Recall: {:.3} Precision: {:.3} Accuracy: {:.3}\".format(bi_f1, bi_recall, bi_precision, bi_acc))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 8))\n",
        "cfm = confusion_matrix(y_true, y_pred) \n",
        "ax = sn.heatmap(cfm, annot=True, annot_kws={\"size\": 10}, fmt='.20g')\n",
        "plt.xlabel(\"predicted label\")\n",
        "plt.ylabel(\"true label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "TnduImKfRFXv",
        "outputId": "4f916af6-e5a5-4a48-c90b-45c84ff09571"
      },
      "id": "TnduImKfRFXv",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary classification: F1-score: 0.412 Recall: 0.497 Precision: 0.352 Accuracy: 0.927\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Stacked-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}